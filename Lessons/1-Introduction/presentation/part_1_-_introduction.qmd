---
title: "Part 1 - Introduction to Machine Learning"
author: "Alexandre Bry"
incremental: true
slide-level: 3
reference-location: document
include-after-body: include-after-body.html
format:
  revealjs:
    # theme: [default, _extensions/grantmcdermott/clean/clean.scss]
    smaller: true
    # scrollable: true
    theme: simple
    slide-number: true
    show-slide-number: all
    css: [styles.css]
    code-overflow: wrap
    code-line-numbers: false
execute: 
  echo: true
---

# Introduction

### Definition

::: {.definition-block}
Machine Learning (ML)

Feeding **data** into a computer **algorithm** in order to learn **patterns** and make **predictions** in new and different **situations**.
:::

### Dataset

# Categories of ML

### Available data

- Supervised: for each input in the dataset, the expected output is also part of the dataset
- Unsupervised: for each input in the dataset, the expected output is **not** part of the dataset
- Semi-supervised: only a portion of the inputs of the dataset have their expected output in the dataset
- Reinforcement: there is no predefined dataset, but an environment giving feedback to the model when it takes actions

### Output

- Classification: assigning one (or multiple) label(s) chosen from a given list of classes to each element of the input
- Regression: assigning one (or multiple) value(s) chosen from a continuous set of values
- Clustering: create categories by grouping together similar inputs

# Overview of ML methods

## Supervised Learning

- **Linear Regression**: Used for predicting continuous values.
- **Logistic Regression**: Used for binary classification problems.
- **Decision Trees**: A tree-like structure used for both classification and regression.
- **Random Forests**: An ensemble method that combines multiple decision trees.
- **Support Vector Machines (SVM)**: Used for classification and regression, effective in high-dimensional spaces.
- **K-Nearest Neighbors (KNN)**: A non-parametric method for classification and regression.
- **Naive Bayes**: A probabilistic classifier based on Bayes' theorem, often used for text classification.
- **Gradient Boosting (e.g., XGBoost, LightGBM)**: Boosting technique that combines weak learners (usually decision trees) to form a stronger model.

## Unsupervised Learning

- **K-Means Clustering**: A method for partitioning data into K clusters.
- **Hierarchical Clustering**: Builds a hierarchy of clusters using either agglomerative or divisive methods.
- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**: Clustering based on the density of data points.
- **Principal Component Analysis (PCA)**: Dimensionality reduction technique to project data into lower dimensions.
- **Independent Component Analysis (ICA)**: Similar to PCA but focuses on maximizing the statistical independence of the components.
- **Autoencoders**: A type of neural network used for unsupervised learning, primarily for dimensionality reduction or feature learning.
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: A nonlinear dimensionality reduction technique primarily used for visualization of high-dimensional data.

## Reinforcement Learning

- **Q-Learning**: A value-based reinforcement learning algorithm.
- **SARSA (State-Action-Reward-State-Action)**: A modification of Q-Learning.
- **Deep Q-Networks (DQN)**: Combines Q-Learning with deep neural networks for environments with large state spaces.
- **Policy Gradient Methods**: These directly optimize the policy by adjusting parameters in the direction that improves performance (e.g., REINFORCE algorithm).
- **Actor-Critic Methods**: Combines policy gradients and value-based methods to optimize both value functions and policies simultaneously.

# Overview of NN methods

## Feedforward Neural Networks (FNN)

The most basic type of neural network where information moves in one direction (input to output).

- **Fully Connected Networks (Dense Networks)**: Every neuron in one layer is connected to every neuron in the next layer.
- **Multilayer Perceptron (MLP)**: A type of fully connected network with one or more hidden layers.

## Convolutional Neural Networks (CNN)

Primarily used for image data, where the convolution operation captures spatial hierarchies in the data.

- **LeNet**: An early CNN model designed for handwritten digit recognition.
- **AlexNet**: A pioneering CNN model in deep learning, which won the ImageNet challenge in 2012.
- **VGGNet**: Known for its deep and uniform architecture (e.g., VGG16, VGG19).
- **ResNet**: A very deep network with skip connections to avoid vanishing gradients, allowing training of networks with hundreds or thousands of layers.
- **Inception Network (GoogLeNet)**: Uses different sized convolution filters in parallel and concatenates their outputs.

## Recurrent Neural Networks (RNN)

Used for sequential data like time series, text, or speech, where the network has loops to allow information persistence.

- **Simple RNN**: Basic form of RNN where each neuron in the current time step is connected to the previous time step.
- **Long Short-Term Memory (LSTM)**: A special type of RNN capable of learning long-term dependencies, overcoming the vanishing gradient problem.
- **Gated Recurrent Unit (GRU)**: A simplified version of LSTM with fewer parameters, designed to improve computational efficiency.
  
## Transformers

Currently the state-of-the-art architecture for natural language processing and other sequential tasks.

- **BERT (Bidirectional Encoder Representations from Transformers)**: A transformer model pre-trained for a wide variety of language tasks.
- **GPT (Generative Pre-trained Transformer)**: A transformer model designed for generating text, famous for its application in tasks like language modeling and dialogue generation.
- **T5 (Text-to-Text Transfer Transformer)**: Converts all NLP tasks into a text-to-text format.
- **Vision Transformers (ViT)**: An adaptation of transformers for image data, treating an image as a sequence of patches.

## Autoencoders

A type of neural network used for unsupervised learning by learning an efficient encoding of input data.

- **Denoising Autoencoders**: Trains the network to remove noise from input data.
- **Variational Autoencoders (VAE)**: A generative model that learns to map data to a continuous latent space, allowing generation of new data points.

## Generative Models

Focused on generating new data from learned distributions.

- **Generative Adversarial Networks (GANs)**: Consist of two networks, a generator and a discriminator, which compete in a game to improve each otherâ€™s performance.
- **Conditional GANs (cGANs)**: GANs conditioned on additional information like class labels.
- **StyleGAN**: A GAN variant that generates highly realistic images with control over image style features.

## Attention Mechanisms

Attention mechanisms are often embedded into neural networks, especially in natural language processing and vision tasks.

- **Self-Attention**: A mechanism in which every part of an input sequence interacts with every other part, learning which parts to focus on (core to transformers).
- **Attention in RNNs**: Used to allow the network to focus on certain parts of a sequence when making predictions.

## Neural Architecture Search (NAS)

Automated methods to discover optimal neural network architectures.

- **EfficientNet**: A family of models designed using NAS techniques, which balance model complexity and performance.

### Playground

[Playground](https://playground.tensorflow.org/)

# Usual pipeline

## Data acquisition

## Data preprocessing

- Outliers
- Missing data
- Formatting
- Normalization

## Model selection

- Type of model
- Complexity
- Parameters

## Model evaluation

- Cross-validation
- Hyperparameter tuning

## Final model training

# Challenges

## Data

- Quality
- Diversity
- Biases and fairness (example of crime prediction)

## Underfitting and Overfitting

- Cross-validation
- Feature selection
- Regularization
- Ensemble methods
- Model complexity (increase if high bias and reduce if high variance)
- Early stopping (overfitting)
- Training data size (underfitting)

## Interpretable and Explainable

## Bias and variance (stability)

### Definitions

::: {.definition-block}
Interpretable

Qualifies a ML model which decision-making process is straightforward and **transparent**, making it directly **understandable** by humans. This requires to restrict the model **complexity**.
:::

::: {.definition-block}
Explainable

Qualifies a ML model which decision-making process can be partly interpreted afterwards using **post hoc interpretation techniques**. 
:::

### Examples

- Example of the model detecting fishes using human hands
- Depends a lot on the technique that is used

# Python libraries

## Data manipulation

### Libraries

::: {.fragment .nonincremental}
- [NumPy](https://numpy.org/)
  - Fast numerical operations
  - Matrices with any number of dimensions (called arrays)
  - Lots of convenient operators on arrays
:::
::: {.fragment .nonincremental}
- [Pandas](https://pandas.pydata.org/)
  - Can store any type of data
  - 1D or 2D tables (called DataFrames)
  - Lots of convenient operators on DataFrames
:::

### NumPy

::: {.fragment}
```{python}
import numpy as np
```
:::
::: {.fragment}
```{python}
array = np.array([[0, 1], [2, 3]])
print(array)
```
:::
::: {.fragment}
```{python}
print(5 * array)
```
:::
::: {.fragment}
```{python}
print(np.pow(array, 3))
```
:::
::: {.fragment}
```{python}
print(array @ array)
```
:::
::: {.fragment}
```{python}
print(np.where(array < 2, 10 - array, array))
```
:::

### Pandas

::: {.fragment}
```{python}
import pandas as pd
import numpy.random as npr
```
:::

::: {.fragment}
```{python}
df = pd.DataFrame([
    ["Pi", 3.14159, npr.randint(-100, 101, (2, 2))],
    ["Euler's number", 2.71828, npr.randint(-100, 101, (2, 2))],
    ["Golden ratio", 1.61803, npr.randint(-100, 101, (2, 2))]
  ], columns = ["Names", "Values", "Random numbers because why not"])
print(df)
```
:::

::: {.fragment}
```{python}
print(df[df["Values"] > 2])
```
:::

::: {.fragment}
```{python}
print(df[df["Names"].str.contains("n")])
```
:::

## ML

- [SciPy](https://scipy.org/)
- [scikit-learn](https://scikit-learn.org/stable/index.html)

## NN

- [PyTorch](https://pytorch.org/)
- [TensorFlow](https://www.tensorflow.org/)
- [Keras](https://keras.io/)

## Visualization

- [Matplotlib](https://matplotlib.org/)
- [Plotly](https://plotly.com/python/)
- [Seaborn](https://seaborn.pydata.org/index.html)

# Resources

## Machine Learning

- GeeksforGeeks:
  - [GeeksforGeeks Introduction to Machine Learning](https://www.geeksforgeeks.org/introduction-machine-learning/)
  - [GeeksforGeeks 7 Major ML Challenges](https://www.geeksforgeeks.org/7-major-challenges-faced-by-machine-learning-professionals/)
  - [GeeksforGeeks Explainable and Interpretable](https://www.geeksforgeeks.org/what-is-the-difference-between-explainable-and-interpretable-machine-learning/)
- [Interpretable Machine Learning, Christoph Molnar](https://christophm.github.io/interpretable-ml-book/)
